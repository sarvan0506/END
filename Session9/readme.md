# Seq2Seq prediction using Encoder-Decoder Architecture and Attention

In this exercise we are going to implement 2 models

1. [Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation](./References/Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb).
2. [Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate](./References/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.ipynb).

on the below mentioned 4 datasets.

## 1. 



